{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook will attempt to build a GPT-2 style transformer model from scratch\n",
    "\n",
    "**Disclaimer** : I have relied on inspiration from Neel Nanda's walkthrough and Anthropic's Transformer Circuits thread. However, this is completely my own implementation.\n",
    "\n",
    "**Author**: Aniruddh Galgali, 2024\n",
    "\n",
    "The 'key' individual components are as follows:\n",
    "\n",
    "1. Embedding (Word and Position Embedding) and Unembedding Layers\n",
    "\n",
    "2. Layer Norm - This occurs before every new layer (i.e either before attention, MLP or unembed)\n",
    "\n",
    "3. A transformer block consisiting of:\n",
    "\n",
    "    i. Self-attention (usually multiple independent heads)\n",
    "\n",
    "    ii. Multi-layer Perceptron \n",
    "\n",
    "The transformer uses a 'residual' type architecture i.e the main information highway is the transformer \"residual stream\" which is of dimensionality 'd_model'. The outputs of each sub-layer (i.e self-attention or MLP) just add back to the original residual stream.\n",
    "\n",
    "The inputs to the model are a series of tokens these are typically an integer representation that are obtained through tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from einops import einsum, rearrange, reduce, repeat\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens.utils import tokenize_and_concatenate, keep_single_column\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Dataset\n",
    "We will use the Tiny Stories dataset that's available on the HuggingFace hub to train our transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"roneneldan/TinyStories\",split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n",
      "Dataset features: {'text': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset:{ds}')\n",
    "print(f'Dataset features: {ds[\"train\"].features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the data is automatically split into a train and validation set. Makes life easy for us! Let's now look at some examples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tiny story : random sample 1 \n",
      "\n",
      "{'text': 'One day, a little girl named Lily found a needle in her room. She '\n",
      "         'knew it was difficult to play with it because it was sharp. Lily '\n",
      "         'wanted to share the needle with her mom, so she could sew a button '\n",
      "         'on her shirt.\\n'\n",
      "         '\\n'\n",
      "         'Lily went to her mom and said, \"Mom, I found this needle. Can you '\n",
      "         'share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, '\n",
      "         'Lily, we can share the needle and fix your shirt.\"\\n'\n",
      "         '\\n'\n",
      "         \"Together, they shared the needle and sewed the button on Lily's \"\n",
      "         'shirt. It was not difficult for them because they were sharing and '\n",
      "         'helping each other. After they finished, Lily thanked her mom for '\n",
      "         'sharing the needle and fixing her shirt. They both felt happy '\n",
      "         'because they had shared and worked together.'}\n",
      " Tiny story : random sample 2 \n",
      "\n",
      "{'text': 'There was a little girl with dark hair. Her name was Joy. She lived '\n",
      "         'in a big house with her parents. One day, Joy was playing outside in '\n",
      "         'her garden. Suddenly, she felt something on her leg - something '\n",
      "         'pinching her. It was a big, black bug! \\n'\n",
      "         '\\n'\n",
      "         'Joy screamed and tried to get away, but the bug kept following her. '\n",
      "         'She tried to run and hide, but it was too quick. \\n'\n",
      "         '\\n'\n",
      "         \"Joy's parents heard her cries and came running. They used a stick to \"\n",
      "         'help her get rid of the bug. After the bug was gone, they hugged Joy '\n",
      "         'and told her everything would be alright. \\n'\n",
      "         '\\n'\n",
      "         'When the bug was gone, Joy felt relieved and happy. She went back to '\n",
      "         \"playing in the garden, making sure she didn't step on any more bugs.\"}\n"
     ]
    }
   ],
   "source": [
    "# Printing some examples\n",
    "print(f' Tiny story : random sample 1 \\n')\n",
    "pprint.pprint(ds['train'][0])\n",
    "print(f' Tiny story : random sample 2 \\n')\n",
    "pprint.pprint(ds['train'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Tokenizer\n",
    "\n",
    "### Let's use the GPT-2 tokenizer to tokenize the above examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='roneneldan/TinyStories-1M', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Using the same tokenizer as GPT2\n",
    "tokenizer_model_name = \"roneneldan/TinyStories-1M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the 'tokenize_and_concatenate' function in the TransformerLens API to actually obtain the tokenized representation of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc5380e8d74478c9365cdc50eeb4519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10666 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12536 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12297 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13147 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "MAX_CTX_LENGTH = 512 # This is the maximum context length\n",
    "tokenized_dataset = tokenize_and_concatenate(ds[\"train\"],tokenizer=tokenizer,max_length= MAX_CTX_LENGTH, num_proc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50256,  3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,\n",
      "          257, 17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,\n",
      "         2408,   284,   711,   351,   340,   780,   340,   373,  7786,    13,\n",
      "        20037,  2227,   284,  2648,   262, 17598,   351,   607,  1995,    11])\n",
      "('<|endoftext|>One day, a little girl named Lily found a needle in her room. '\n",
      " 'She knew it was difficult to play with it because it was sharp. Lily wanted '\n",
      " 'to share the needle with her mom,')\n"
     ]
    }
   ],
   "source": [
    "# Looking at some tokens\n",
    "pprint.pprint(tokenized_dataset['tokens'][0,:40])\n",
    "\n",
    "# Looking at whether tokens corrrespond to the correct part of text\n",
    "pprint.pprint(tokenizer.decode(tokenized_dataset['tokens'][0,:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "data_loader = DataLoader(tokenized_dataset,batch_size=BATCH_SIZE, shuffle=True,pin_memory=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a config dataclass that contains all the hyper-parameters\n",
    "@dataclass\n",
    "class Config:\n",
    "    # These numbers are not from the standard implementation of GTP-2. Instead \n",
    "    # most numbers are much smaller due to training resource constraints. Only\n",
    "    # the d_vocab is consistent, as without that the tokenizer will not work.\n",
    "    d_model = 256\n",
    "    d_head = 64\n",
    "    n_heads = 5\n",
    "    d_mlp = 1024\n",
    "    d_vocab = 50257\n",
    "    layer_norm_eps = 1e-5\n",
    "    init_std = 0.02\n",
    "    max_ctx = MAX_CTX_LENGTH\n",
    "    batch_size = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Embedding and Unembedding layers\n",
    "\n",
    "This include the embedding layer, the positional enmbedding and the final unembedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    \"\"\" \n",
    "        Embedding layer that takes as inputs a batch of \n",
    "        token and embeds them into vectors of size d_model.\n",
    "        This is the first layer after the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_vocab, d_model, init_std = 0.02):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Parameter(torch.empty(d_vocab, d_model)) # embedding matrix\n",
    "        nn.init.normal_(self.W_E, std=init_std)\n",
    "        print(self.W_E.shape)\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        # input_tokens are of size [batch_size, position] and are \n",
    "        # essentially integers that index the rows of W_E\n",
    "\n",
    "        embedded = self.W_E[input_tokens,:]\n",
    "        return embedded\n",
    "    \n",
    "class PositionEmbedding(nn.Module):\n",
    "    \"\"\" \n",
    "    \n",
    "        This layer produces the relevant positional information.\n",
    "        This positional information is then added to the output of\n",
    "        the Embedding layer resulting in the input to the first\n",
    "        transformer block \n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, max_ctx, d_model, init_std = 0.02):\n",
    "        super().__init__()\n",
    "        self.W_P = nn.Parameter(torch.empty(max_ctx, d_model))\n",
    "        nn.init.normal_(self.W_P, std = init_std)\n",
    "        print(self.W_P.shape)\n",
    "\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        #input_tokens are of size [batch, position]\n",
    "        pos_embed = repeat(self.W_P[:input_tokens.shape[1],:], \"pos d_model -> batch pos d_model\", batch = input_tokens.shape[0])\n",
    "        print(pos_embed.shape)\n",
    "        return pos_embed\n",
    "    \n",
    "class UnembeddingLayer(nn.Module):\n",
    "    \"\"\" Unembedding layer that takes as inputs the ouput\n",
    "        from the last transformer block and expands it back\n",
    "        to a vector of size d_vocab, which are the logits that\n",
    "        get passed on to the Softmax. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_vocab, d_model, init_std):\n",
    "        super().__init__()\n",
    "        self.W_U = nn.Parameter(torch.empty(d_model,d_vocab))\n",
    "        nn.init.normal_(self.W_U, std = init_std)\n",
    "        self.b_U = nn.Parameter(torch.zeros(d_vocab,))\n",
    "\n",
    "    def forward(self, resid_embed_last):\n",
    "        # resid_embed_last corresponds to the residual stream\n",
    "        # after the last transformer block. It is of size \n",
    "        # [batch_size, position, d_model] \n",
    "\n",
    "        logits = einsum(resid_embed_last, self.W_U,  \"batch pos d_model, d_model d_vocab -> batch pos d_vocab\") + self.b_U\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Layer Norm\n",
    "\n",
    "Note that using the in-built nn.LayerNorm() is maybe more conveneint. But here, I have written my own, for learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    \"\"\" Layer Normalization. Effectively z-scores it's input\n",
    "        along the embedding dimension and then multiplies \n",
    "        each embedding dimension independently by learnable\n",
    "        gains\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, init_std = 0.02, layer_norm_eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.gains = nn.Parameter(torch.empty(d_model,))\n",
    "        nn.init.normal_(self.gains,std = init_std)\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model,))\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Input is the residual stram and is a tensor of \n",
    "        # size [batch, position, d_model]. The layer norm\n",
    "        # subtracts the mean and variance computed across the\n",
    "        # d_model dimension\n",
    "        layer_mean = input.mean(dim=-1,keepdim=True)\n",
    "        input_centered = (input - layer_mean)\n",
    "        layer_var = input_centered.var(dim=-1,keepdim=True)\n",
    "        layer_scale = torch.sqrt(layer_var + self.layer_norm_eps) \n",
    "        input_normalized = input_centered/layer_scale\n",
    "        output = input_normalized *self.gains + self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    \"\"\" This module implements the self-attention mechanism with\n",
    "     multiple heads assuming that there are a total of \"n_heads\" heads.\n",
    "    The forward() method of this class provides the output of the\n",
    "    attention module which is a weighted combination of value vectors,\n",
    "    where the weights are the \"attention weights\" \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, d_model, d_head, mask_val = -1e5, init_std=0.02):\n",
    "        super().__init__()\n",
    "        self.W_Q = torch.nn.Parameter(torch.empty(n_heads, d_model, d_head))\n",
    "        nn.init.normal_(self.W_Q,std = init_std)\n",
    "        self.b_Q = torch.nn.Parameter(torch.zeros(n_heads, d_head))\n",
    "        self.W_K = torch.nn.Parameter(torch.empty(n_heads, d_model, d_head))\n",
    "        nn.init.normal_(self.W_K,std = init_std)\n",
    "        self.b_K = torch.nn.Parameter(torch.zeros(n_heads, d_head))\n",
    "        self.W_V = torch.nn.Parameter(torch.empty(n_heads, d_model, d_head))\n",
    "        nn.init.normal_(self.W_V,std = init_std)\n",
    "        self.b_V = torch.nn.Parameter(torch.zeros(n_heads, d_head))\n",
    "        self.W_O = torch.nn.Parameter(torch.empty(n_heads, d_head, d_model))\n",
    "        nn.init.normal_(self.W_O,std = init_std)\n",
    "        self.b_O = torch.nn.Parameter(torch.zeros(d_model))\n",
    "        self.register_buffer(\"mask_val\",torch.tensor(mask_val,dtype = torch.float32))\n",
    "\n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre is of size [batch, position, d_model]\n",
    "        \n",
    "        # Computing the keys, queries and values \n",
    "        keys = einsum(resid_pre, self.W_K, \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\") + self.b_K\n",
    "        queries = einsum(resid_pre, self.W_Q, \"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\") + self.b_Q\n",
    "        values = einsum(resid_pre, self.W_V, \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\") + self.b_V\n",
    "        \n",
    "        # Computing the attention pattern\n",
    "        attn_pattern = einsum(queries, keys, \"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\") \n",
    "        attn_pattern /= math.sqrt(self.W_Q.shape[-1])\n",
    "        mask = torch.triu(torch.ones(attn_pattern.shape[-2], attn_pattern.shape[-1]),diagonal=1).bool()\n",
    "        attn_pattern.masked_fill_(mask, self.mask_val)\n",
    "        attn_pattern = nn.Softmax(dim=-1)(attn_pattern)\n",
    "        context_vec = einsum(attn_pattern, values, \"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch n_heads query_pos d_head\")\n",
    "\n",
    "        # Note that in the original formulation of the transformer, one concatenates the outputs of the heads and then multiplies by a W_O that has \n",
    "        # block rows equivalent to the individual W_O for each head. This is done for efficiency purposes. Therefore, my implementation below is \n",
    "        # not efficient, but is only for didactic clarity.\n",
    "\n",
    "        # Output : Sum of all attention heads (see comment above)\n",
    "        output = einsum(context_vec, self.W_O, \"batch n_heads query_pos d_head, n_heads d_head d_model -> batch query_pos d_model\") + self.b_O\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu_new(input):\n",
    "    \"\"\"\n",
    "    This is the activation function used by GPT-2. Apparently,\n",
    "    it's slightly different from PyTorch's nn.gelu() implementation\n",
    "    \"\"\"\n",
    "    return 0.5*input* (1.0 + torch.tanh(math.sqrt(2.0/math.pi) * \n",
    "                        (input + 0.04715 *torch.pow(input, 3.0))))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_mlp, d_model):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(d_model, d_mlp)\n",
    "        self.output_layer = nn.Linear(d_mlp, d_model)\n",
    "        self.act_fn = gelu_new\n",
    "\n",
    "    def forward(self, resid_attended):\n",
    "        pre_act = self.input_layer(resid_attended)\n",
    "        act = self.act_fn(pre_act)\n",
    "        output = self.output_layer(act)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_mlp, d_head, n_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(d_model =d_model)\n",
    "        self.attn = SelfAttention(n_heads, d_model, d_head)\n",
    "        self.ln2 = LayerNorm(d_model =d_model)\n",
    "        self.mlp = MLP(d_mlp, d_model) \n",
    "\n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre is of size [batch, position, d_model]\n",
    "        ln1_out = self.ln1(resid_pre)\n",
    "        attn_out = self.attn(ln1_out)\n",
    "        resid_mid = resid_pre +  attn_out\n",
    "        ln2_out = self.ln2(resid_mid)\n",
    "        resid_post = resid_mid + self.mlp(ln2_out)\n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed = EmbeddingLayer(config.d_vocab, config.d_model, config.init_std)\n",
    "        self.pos_embed = PositionEmbedding(config.max_ctx, config.d_model, config.init_std)\n",
    "        self.transformer = nn.ModuleList([TransformerBlock(config.d_model, config.d_mlp,\n",
    "                                config.d_head, config.n_heads) for _ in config.num_blocks])\n",
    "        self.ln = LayerNorm(config.d_model, config.init_std, config.layer_norm_eps)\n",
    "        self.unembed = UnembeddingLayer(config.d_vocab, config.d_model, config.init_std)\n",
    "\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        embedded = self.embed(tokens)\n",
    "        pos_embed = self.pos_embed(tokens)\n",
    "        resid = embedded + pos_embed\n",
    "        for i in range(len(self.transformer)):\n",
    "            resid = self.transformer[i](resid)\n",
    "        resid_normalized_final = self.ln(resid)\n",
    "        logits = self.unembed(resid_normalized_final)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_lm(logits, tokens):\n",
    "    # tokens of size [batch, pos]\n",
    "    # logits of size [batch, pos, d_vocab]\n",
    "\n",
    "    log_probs = nn.LogSoftmax(dim=-1)(logits)\n",
    "    pred_log_probs = torch.gather(log_probs[:,:-1,:],dim=-1, index = tokens[:,1:].unsqueeze(dim=-1)).squeeze(dim=-1)\n",
    "    return pred_log_probs.mean(dim=-1)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechinterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
