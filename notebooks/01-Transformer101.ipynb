{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook will attempt to build a GPT-2 style transformer model from scratch\n",
    "\n",
    "**Disclaimer** : I have relied on inspiration from Neel Nanda's walkthrough and Anthropic's Transformer Circuits thread. However, this is completely my own implementation.\n",
    "\n",
    "**Author**: Aniruddh Galgali, 2024\n",
    "\n",
    "The 'key' individual components are as follows:\n",
    "\n",
    "1. Embedding (Word and Position Embedding) and Unembedding Layers\n",
    "\n",
    "2. Layer Norm - This occurs before every new layer (i.e either before attention, MLP or unembed)\n",
    "\n",
    "3. A transformer block consisiting of:\n",
    "\n",
    "    i. Self-attention (usually multiple independent heads)\n",
    "\n",
    "    ii. Multi-layer Perceptron \n",
    "\n",
    "The transformer uses a 'residual' type architecture i.e the main information highway is the transformer \"residual stream\" which is of dimensionality 'd_model'. The outputs of each sub-layer (i.e self-attention or MLP) just add back to the original residual stream.\n",
    "\n",
    "The inputs to the model are a series of tokens these are typically an integer representation that are obtained through tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from einops import einsum, rearrange, reduce, repeat\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens.utils import tokenize_and_concatenate, keep_single_column\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Dataset\n",
    "We will use the Tiny Stories dataset that's available on the HuggingFace hub to train our transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3fbf30e9104db58f8ed420777dfe23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c70fd33e914711bda5d4b8dd01834b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31eedbaf6314b809cfeb8c9ea44b951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91656c509e9c4ddc89613d4856b5680b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4443b358439b41229ba8a4f97ab2e967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d387df9c1b1640f982884524c4396654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc9d603f9c34d4dbb0fa6252fd60498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2972725cf945a69a40732a87d39caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n",
      "Dataset features: {'text': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset:{ds}')\n",
    "print(f'Dataset features: {ds[\"train\"].features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the data is automatically split into a train and validation set. Makes life easy for us! Let's now look at some examples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tiny story : random sample 1 \n",
      "\n",
      "{'text': 'One day, a little girl named Lily found a needle in her room. She '\n",
      "         'knew it was difficult to play with it because it was sharp. Lily '\n",
      "         'wanted to share the needle with her mom, so she could sew a button '\n",
      "         'on her shirt.\\n'\n",
      "         '\\n'\n",
      "         'Lily went to her mom and said, \"Mom, I found this needle. Can you '\n",
      "         'share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, '\n",
      "         'Lily, we can share the needle and fix your shirt.\"\\n'\n",
      "         '\\n'\n",
      "         \"Together, they shared the needle and sewed the button on Lily's \"\n",
      "         'shirt. It was not difficult for them because they were sharing and '\n",
      "         'helping each other. After they finished, Lily thanked her mom for '\n",
      "         'sharing the needle and fixing her shirt. They both felt happy '\n",
      "         'because they had shared and worked together.'}\n",
      " Tiny story : random sample 2 \n",
      "\n",
      "{'text': 'Once upon a time, there was a little car named Beep. Beep loved to '\n",
      "         'go fast and play in the sun. Beep was a healthy car because he '\n",
      "         'always had good fuel. Good fuel made Beep happy and strong.\\n'\n",
      "         '\\n'\n",
      "         'One day, Beep was driving in the park when he saw a big tree. The '\n",
      "         'tree had many leaves that were falling. Beep liked how the leaves '\n",
      "         'fall and wanted to play with them. Beep drove under the tree and '\n",
      "         'watched the leaves fall on him. He laughed and beeped his horn.\\n'\n",
      "         '\\n'\n",
      "         'Beep played with the falling leaves all day. When it was time to go '\n",
      "         'home, Beep knew he needed more fuel. He went to the fuel place and '\n",
      "         'got more healthy fuel. Now, Beep was ready to go fast and play again '\n",
      "         'the next day. And Beep lived happily ever after.'}\n"
     ]
    }
   ],
   "source": [
    "# Printing some examples\n",
    "print(f' Tiny story : random sample 1 \\n')\n",
    "pprint.pprint(ds['train'][0])\n",
    "print(f' Tiny story : random sample 2 \\n')\n",
    "pprint.pprint(ds['train'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Tokenizer\n",
    "\n",
    "### Let's use the GPT-2 tokenizer to tokenize the above examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Using the same tokenizer as GPT2\n",
    "tokenizer_model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the 'tokenize_and_concatenate' function in the TransformerLens API to actually obtain the tokenized representation of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenize_and_concatenate(dataset,tokenizer,streaming = False, max_length = 1024,\n",
    "            column_name = \"text\", add_bos_token = True,num_proc = 10):\n",
    "\n",
    "\n",
    "    dataset = keep_single_column(dataset, column_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        # We add a padding token, purely to implement the tokenizer. This will be removed before inputting tokens to the model, so we do not need to increment d_vocab in the model.\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "    # Define the length to chop things up into - leaving space for a bos_token if required\n",
    "    if add_bos_token:\n",
    "        seq_len = max_length - 1\n",
    "    else:\n",
    "        seq_len = max_length\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        text = examples[column_name]\n",
    "        # Concatenate it all into an enormous string, separated by eos_tokens\n",
    "        full_text = tokenizer.eos_token.join(text)\n",
    "\n",
    "        # Handle the case when full_text is empty\n",
    "        if not full_text.strip():\n",
    "            return {\"tokens\": np.array([], dtype=np.int64)}\n",
    "\n",
    "        # Divide into 20 chunks of ~ equal length\n",
    "        num_chunks = 20\n",
    "        chunk_length = (len(full_text) - 1) // num_chunks + 1\n",
    "        chunks = [full_text[i * chunk_length : (i + 1) * chunk_length] for i in range(num_chunks)]\n",
    "        # Tokenize the chunks in parallel. Uses NumPy because HuggingFace map doesn't want tensors returned\n",
    "        tokens = tokenizer(chunks, return_tensors=\"np\", padding=True)[\"input_ids\"].flatten()\n",
    "        # Drop padding tokens\n",
    "        tokens = tokens[tokens != tokenizer.pad_token_id]\n",
    "        num_tokens = len(tokens)\n",
    "\n",
    "        # Handle cases where num_tokens is less than seq_len\n",
    "        if num_tokens < seq_len:\n",
    "            num_batches = 1\n",
    "            # Pad tokens if necessary\n",
    "            tokens = tokens[:seq_len]\n",
    "            if len(tokens) < seq_len:\n",
    "                padding_length = seq_len - len(tokens)\n",
    "                padding = np.full(padding_length, tokenizer.pad_token_id)\n",
    "                tokens = np.concatenate([tokens, padding], axis=0)\n",
    "        else:\n",
    "            num_batches = num_tokens // seq_len\n",
    "            # Drop the final tokens if not enough to make a full sequence\n",
    "            tokens = tokens[: seq_len * num_batches]\n",
    "\n",
    "        tokens = rearrange(\n",
    "            tokens, \"(batch seq) -> batch seq\", batch=num_batches, seq=seq_len\n",
    "        )\n",
    "        if add_bos_token:\n",
    "            prefix = np.full((num_batches, 1), tokenizer.bos_token_id)\n",
    "            tokens = np.concatenate([prefix, tokens], axis=1)\n",
    "        return {\"tokens\": tokens}\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=(num_proc if not streaming else None),\n",
    "        remove_columns=[column_name],\n",
    "    )\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"tokens\"])\n",
    "    return tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47229f22576c4d83a1bc293ca3551b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10666 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12536 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12297 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13147 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 1024 # This is the maximum context length\n",
    "tokenized_dataset = tokenize_and_concatenate(ds[\"train\"],tokenizer=tokenizer,max_length= MAX_LENGTH, num_proc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O<|endoftext|>n<|endoftext|>e<|endoftext|> <|endoftext|>d<|endoftext|>a<|endoftext|>y<|endoftext|>,<|endoftext|> <|endoftext|>a<|endoftext|> <|endoftext|>l<|endoftext|>i<|endoftext|>t<|endoftext|>t<|endoftext|>l<|endoftext|>e<|endoftext|> <|endoftext|>g<|endoftext|>i<|endoftext|>r<|endoftext|>l<|endoftext|> <|endoftext|>n<|endoftext|>a<|endoftext|>m<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>L<|endoftext|>i<|endoftext|>l<|endoftext|>y<|endoftext|> <|endoftext|>f<|endoftext|>o<|endoftext|>u<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>a<|endoftext|> <|endoftext|>n<|endoftext|>e<|endoftext|>e<|endoftext|>d<|endoftext|>l<|endoftext|>e<|endoftext|> <|endoftext|>i<|endoftext|>n<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>r<|endoftext|>o<|endoftext|>o<|endoftext|>m<|endoftext|>.<|endoftext|> <|endoftext|>S<|endoftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>k<|endoftext|>n<|endoftext|>e<|endoftext|>w<|endoftext|> <|endoftext|>i<|endoftext|>t<|endoftext|> <|endoftext|>w<|endoftext|>a<|endoftext|>s<|endoftext|> <|endoftext|>d<|endoftext|>i<|endoftext|>f<|endoftext|>f<|endoftext|>i<|endoftext|>c<|endoftext|>u<|endoftext|>l<|endoftext|>t<|endoftext|> <|endoftext|>t<|endoftext|>o<|endoftext|> <|endoftext|>p<|endoftext|>l<|endoftext|>a<|endoftext|>y<|endoftext|> <|endoftext|>w<|endoftext|>i<|endoftext|>t<|endoftext|>h<|endoftext|> <|endoftext|>i<|endoftext|>t<|endoftext|> <|endoftext|>b<|endoftext|>e<|endoftext|>c<|endoftext|>a<|endoftext|>u<|endoftext|>s<|endoftext|>e<|endoftext|> <|endoftext|>i<|endoftext|>t<|endoftext|> <|endoftext|>w<|endoftext|>a<|endoftext|>s<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>p<|endoftext|>.<|endoftext|> <|endoftext|>L<|endoftext|>i<|endoftext|>l<|endoftext|>y<|endoftext|> <|endoftext|>w<|endoftext|>a<|endoftext|>n<|endoftext|>t<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>t<|endoftext|>o<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>e<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>n<|endoftext|>e<|endoftext|>e<|endoftext|>d<|endoftext|>l<|endoftext|>e<|endoftext|> <|endoftext|>w<|endoftext|>i<|endoftext|>t<|endoftext|>h<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>m<|endoftext|>o<|endoftext|>m<|endoftext|>,<|endoftext|> <|endoftext|>s<|endoftext|>o<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>c<|endoftext|>o<|endoftext|>u<|endoftext|>l<|endoftext|>d<|endoftext|> <|endoftext|>s<|endoftext|>e<|endoftext|>w<|endoftext|> <|endoftext|>a<|endoftext|> <|endoftext|>b<|endoftext|>u<|endoftext|>t<|endoftext|>t<|endoftext|>o<|endoftext|>n<|endoftext|> <|endoftext|>o<|endoftext|>n<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>i<|endoftext|>r<|endoftext|>t<|endoftext|>.<|endoftext|>\n",
      "<|endoftext|>\n",
      "<|endoftext|>L<|endoftext|>i<|endoftext|>l<|endoftext|>y<|endoftext|> <|endoftext|>w<|endoftext|>e<|endoftext|>n<|endoftext|>t<|endoftext|> <|endoftext|>t<|endoftext|>o<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>m<|endoftext|>o<|endoftext|>m<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>s<|endoftext|>a<|endoftext|>i<|endoftext|>d<|endoftext|>,<|endoftext|> <|endoftext|>\"<|endoftext|>M<|endoftext|>o<|endoftext|>m<|endoftext|>,<|endoftext|> <|endoftext|>I<|endoftext|> <|endoftext|>f<|endoftext|>o<|endoftext|>u<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>i<|endoftext|>s<|endoftext|> <|endoftext|>n<|endoftext|>e<|endoftext|>e<|endoftext|>d<|endoftext|>l<|endoftext|>e<|endoftext|>.<|endoftext|> <|endoftext|>C<|endoftext|>a<|endoftext|>n<|endoftext|> <|endoftext|>y<|endoftext|>o<|endoftext|>u<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>e<|endoftext|> <|endoftext|>i<|endoftext|>t<|endoftext|> <|endoftext|>w<|endoftext|>i<|endoftext|>t<|endoftext|>h<|endoftext|> <|endoftext|>m<|endoftext|>e<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>s<|endoftext|>e<|endoftext|>w<|endoftext|> <|endoftext|>m<|endoftext|>y<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>i<|endoftext|>r<|endoftext|>t<|endoftext|>?<|endoftext|>\"<|endoftext|> <|endoftext|>H<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>m<|endoftext|>o<|endoftext|>m<|endoftext|> <|endoftext|>s<|endoftext|>m<|endoftext|>i<|endoftext|>l<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>s<|endoftext|>a<|endoftext|>i<|endoftext|>d<|endoftext|>,<|endoftext|> <|endoftext|>\"<|endoftext|>Y<|endoftext|>e<|endoftext|>s<|endoftext|>,<|endoftext|> <|endoftext|>L<|endoftext|>i<|endoftext|>l<|endoftext|>y<|endoftext|>,<|endoftext|> <|endoftext|>w<|endoftext|>e<|endoftext|> <|endoftext|>c<|endoftext|>a<|endoftext|>n<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>e<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>n<|endoftext|>e<|endoftext|>e<|endoftext|>d<|endoftext|>l<|endoftext|>e<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>f<|endoftext|>i<|endoftext|>x<|endoftext|> <|endoftext|>y<|endoftext|>o<|endoftext|>u<|endoftext|>r<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>i<|endoftext|>r<|endoftext|>t<|endoftext|>.<|endoftext|>\"<|endoftext|>\n",
      "<|endoftext|>\n",
      "<|endoftext|>T<|endoftext|>o<|endoftext|>g<|endoftext|>e<|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|>,<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>y<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>n<|endoftext|>e<|endoftext|>e<|endoftext|>d<|endoftext|>l<|endoftext|>e<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>s<|endoftext|>e<|endoftext|>w<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>b<|endoftext|>u<|endoftext|>t<|endoftext|>t<|endoftext|>o<|endoftext|>n<|endoftext|> <|endoftext|>o<|endoftext|>n<|endoftext|> <|endoftext|>L<|endoftext|>i<|endoftext|>l<|endoftext|>y<|endoftext|>'<|endoftext|>s<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>i<|endoftext|>r<|endoftext|>t<|endoftext|>.<|endoftext|> <|endoftext|>I<|endoftext|>t<|endoftext|> <|endoftext|>w<|endoftext|>a<|endoftext|>s<|endoftext|> <|endoftext|>n<|endoftext|>o<|endoftext|>t<|endoftext|> <|endoftext|>d<|endoftext|>i<|endoftext|>f<|endoftext|>f<|endoftext|>i<|endoftext|>c<|endoftext|>u<|endoftext|>l<|endoftext|>t<|endoftext|> <|endoftext|>f<|endoftext|>o<|endoftext|>r<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>m<|endoftext|> <|endoftext|>b<|endoftext|>e<|endoftext|>c<|endoftext|>a<|endoftext|>u<|endoftext|>s<|endoftext|>e<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>y<|endoftext|> <|endoftext|>w<|endoftext|>e<|endoftext|>r<|endoftext|>e<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>i<|endoftext|>n<|endoftext|>g<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>l<|endoftext|>p<|endoftext|>i<|endoftext|>n<|endoftext|>g<|endoftext|> <|endoftext|>e<|endoftext|>a<|endoftext|>c<|endoftext|>h<|endoftext|> <|endoftext|>o<|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|>.<|endoftext|> <|endoftext|>A<|endoftext|>f<|endoftext|>t<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>y<|endoftext|> <|endoftext|>f<|endoftext|>i<|endoftext|>n<|endoftext|>i<|endoftext|>s<|endoftext|>h<|endoftext|>e<|endoftext|>d<|endoftext|>,<|endoftext|> <|endoftext|>L<|endoftext|>i<|endoftext|>l<|endoftext|>y<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>a<|endoftext|>n<|endoftext|>k<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>m<|endoftext|>o<|endoftext|>m<|endoftext|> <|endoftext|>f<|endoftext|>o<|endoftext|>r<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>i<|endoftext|>n<|endoftext|>g<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>n<|endoftext|>e<|endoftext|>e<|endoftext|>d<|endoftext|>l<|endoftext|>e<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>f<|endoftext|>i<|endoftext|>x<|endoftext|>i<|endoftext|>n<|endoftext|>g<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>i<|endoftext|>r<|endoftext|>t<|endoftext|>.<|endoftext|> <|endoftext|>T<|endoftext|>h<|endoftext|>e<|endoftext|>y<|endoftext|> <|endoftext|>b<|endoftext|>o<|endoftext|>t<|endoftext|>h<|endoftext|> <|endoftext|>f<|endoftext|>e<|endoftext|>l<|endoftext|>t<|endoftext|> <|endoftext|>h<|endoftext|>a<|endoftext|>p<|endoftext|>p<|endoftext|>y<|endoftext|> <|endoftext|>b<|endoftext|>e<|endoftext|>c<|endoftext|>a<|endoftext|>u<|endoftext|>s<|endoftext|>e<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>y<|endoftext|> <|endoftext|>h<|endoftext|>a<|endoftext|>d<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>w<|endoftext|>o<|endoftext|>r<|endoftext|>k<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>t<|endoftext|>o<|endoftext|>g<|endoftext|>e<|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|>.\n"
     ]
    }
   ],
   "source": [
    "full_text = tokenizer.eos_token.join(ds['train'][0][\"text\"])\n",
    "num_chunks = 20\n",
    "print(full_text)\n",
    "chunk_length = (len(full_text) - 1) // num_chunks + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "print(chunk_length)\n",
    "chunks = [full_text[i * chunk_length : (i + 1) * chunk_length] for i in range(num_chunks)]\n",
    "        # Tokenize the chunks in parallel. Uses NumPy because HuggingFace map doesn't want tensors returned\n",
    "tokens = tokenizer(chunks, return_tensors=\"np\", padding=True)[\"input_ids\"].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257, 17598, 287, 607, 2119, 13, 1375, 2993, 340, 373, 2408, 284, 711, 351, 340, 780, 340, 373, 7786, 13, 20037, 2227, 284, 2648, 262, 17598, 351, 607, 1995, 11, 523, 673, 714, 34249, 257, 4936, 319, 607, 10147, 13, 198, 198, 43, 813, 1816, 284, 607, 1995, 290, 531, 11, 366, 29252, 11, 314, 1043, 428, 17598, 13, 1680, 345, 2648, 340, 351, 502, 290, 34249, 616, 10147, 1701, 2332, 1995, 13541, 290, 531, 11, 366, 5297, 11, 20037, 11, 356, 460, 2648, 262, 17598, 290, 4259, 534, 10147, 526, 198, 198, 41631, 11, 484, 4888, 262, 17598, 290, 384, 19103, 262, 4936, 319, 20037, 338, 10147, 13, 632, 373, 407, 2408, 329, 606, 780, 484, 547, 7373, 290, 5742, 1123, 584, 13, 2293, 484, 5201, 11, 20037, 26280, 607, 1995, 329, 7373, 262, 17598, 290, 18682, 607, 10147, 13, 1119, 1111, 2936, 3772, 780, 484, 550, 4888, 290, 3111, 1978, 13]\n"
     ]
    }
   ],
   "source": [
    "raw_encodings = tokenizer.encode(ds['train'][0]['text'])\n",
    "print(raw_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O<|endoftext|>n<|endoftext|>e<|endoftext|> <|endoftext|>d<|endoftext|>a<|endoftext|>y<|endoftext|>,<|endoftext|> <|endoftext|>a<|endoftext|> <|endoftext|>l<|endoftext|>i<|endoftext|>t<|endoftext|>t<|endoftext|>l<|endoftext|>e<|endoftext|> <|endoftext|>g<|endoftext|>i<|endoftext|>r<|endoftext|>l<|endoftext|> <|endoftext|>n<|endoftext|>a<|endoftext|>m<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>L<|endoftext|>i<|endoftext|>l<|endoftext|>y<|endoftext|> <|endoftext|>f<|endoftext|>o',\n",
       " '<|endoftext|>u<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>a<|endoftext|> <|endoftext|>n<|endoftext|>e<|endoftext|>e<|endoftext|>d<|endoftext|>l<|endoftext|>e<|endoftext|> <|endoftext|>i<|endoftext|>n<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>r<|endoftext|>o<|endoftext|>o<|endoftext|>m<|endoftext|>.<|endoftext|> <|endoftext|>S<|endoftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>k<|endoftext|>n<|endoftext|>e<|endoftext|>w<|endoftext|> <',\n",
       " '|endoftext|>i<|endoftext|>t<|endoftext|> <|endoftext|>w<|endoftext|>a<|endoftext|>s<|endoftext|> <|endoftext|>d<|endoftext|>i<|endoftext|>f<|endoftext|>f<|endoftext|>i<|endoftext|>c<|endoftext|>u<|endoftext|>l<|endoftext|>t<|endoftext|> <|endoftext|>t<|endoftext|>o<|endoftext|> <|endoftext|>p<|endoftext|>l<|endoftext|>a<|endoftext|>y<|endoftext|> <|endoftext|>w<|endoftext|>i<|endoftext|>t<|endoftext|>h<|endoftext|> <|endoftext|>i<|endoftext|>t<|endoftext|> <|endoftext|>b<|endoftext|>e<|',\n",
       " 'endoftext|>c<|endoftext|>a<|endoftext|>u<|endoftext|>s<|endoftext|>e<|endoftext|> <|endoftext|>i<|endoftext|>t<|endoftext|> <|endoftext|>w<|endoftext|>a<|endoftext|>s<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>p<|endoftext|>.<|endoftext|> <|endoftext|>L<|endoftext|>i<|endoftext|>l<|endoftext|>y<|endoftext|> <|endoftext|>w<|endoftext|>a<|endoftext|>n<|endoftext|>t<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>t<|endoftext|>o<|endoftext|> <|e',\n",
       " 'ndoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>e<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>n<|endoftext|>e<|endoftext|>e<|endoftext|>d<|endoftext|>l<|endoftext|>e<|endoftext|> <|endoftext|>w<|endoftext|>i<|endoftext|>t<|endoftext|>h<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>m<|endoftext|>o<|endoftext|>m<|endoftext|>,<|endoftext|> <|endoftext|>s<|endoftext|>o<|endoftext|> <|endoftext|>s<|en',\n",
       " 'doftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>c<|endoftext|>o<|endoftext|>u<|endoftext|>l<|endoftext|>d<|endoftext|> <|endoftext|>s<|endoftext|>e<|endoftext|>w<|endoftext|> <|endoftext|>a<|endoftext|> <|endoftext|>b<|endoftext|>u<|endoftext|>t<|endoftext|>t<|endoftext|>o<|endoftext|>n<|endoftext|> <|endoftext|>o<|endoftext|>n<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>i<|endoftext|>r<|endoftext|>t<|endoftext|>.<|end',\n",
       " 'oftext|>\\n<|endoftext|>\\n<|endoftext|>L<|endoftext|>i<|endoftext|>l<|endoftext|>y<|endoftext|> <|endoftext|>w<|endoftext|>e<|endoftext|>n<|endoftext|>t<|endoftext|> <|endoftext|>t<|endoftext|>o<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>m<|endoftext|>o<|endoftext|>m<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>s<|endoftext|>a<|endoftext|>i<|endoftext|>d<|endoftext|>,<|endoftext|> <|endoftext|>\"<|endoftext|>M<|endo',\n",
       " 'ftext|>o<|endoftext|>m<|endoftext|>,<|endoftext|> <|endoftext|>I<|endoftext|> <|endoftext|>f<|endoftext|>o<|endoftext|>u<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>i<|endoftext|>s<|endoftext|> <|endoftext|>n<|endoftext|>e<|endoftext|>e<|endoftext|>d<|endoftext|>l<|endoftext|>e<|endoftext|>.<|endoftext|> <|endoftext|>C<|endoftext|>a<|endoftext|>n<|endoftext|> <|endoftext|>y<|endoftext|>o<|endoftext|>u<|endoftext|> <|endoftext|>s<|endoftext|>h<|endof',\n",
       " 'text|>a<|endoftext|>r<|endoftext|>e<|endoftext|> <|endoftext|>i<|endoftext|>t<|endoftext|> <|endoftext|>w<|endoftext|>i<|endoftext|>t<|endoftext|>h<|endoftext|> <|endoftext|>m<|endoftext|>e<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>s<|endoftext|>e<|endoftext|>w<|endoftext|> <|endoftext|>m<|endoftext|>y<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>i<|endoftext|>r<|endoftext|>t<|endoftext|>?<|endoftext|>\"<|endoftext|> <|endoftext|>H<|endoft',\n",
       " 'ext|>e<|endoftext|>r<|endoftext|> <|endoftext|>m<|endoftext|>o<|endoftext|>m<|endoftext|> <|endoftext|>s<|endoftext|>m<|endoftext|>i<|endoftext|>l<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>s<|endoftext|>a<|endoftext|>i<|endoftext|>d<|endoftext|>,<|endoftext|> <|endoftext|>\"<|endoftext|>Y<|endoftext|>e<|endoftext|>s<|endoftext|>,<|endoftext|> <|endoftext|>L<|endoftext|>i<|endoftext|>l<|endoftext|>y<|endoftext|>,<|endofte',\n",
       " 'xt|> <|endoftext|>w<|endoftext|>e<|endoftext|> <|endoftext|>c<|endoftext|>a<|endoftext|>n<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>e<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>n<|endoftext|>e<|endoftext|>e<|endoftext|>d<|endoftext|>l<|endoftext|>e<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>f<|endoftext|>i<|endoftext|>x<|endoftext|> <|endoftext|>y<|endoftext|>o<|endoftex',\n",
       " 't|>u<|endoftext|>r<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>i<|endoftext|>r<|endoftext|>t<|endoftext|>.<|endoftext|>\"<|endoftext|>\\n<|endoftext|>\\n<|endoftext|>T<|endoftext|>o<|endoftext|>g<|endoftext|>e<|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|>,<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>y<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>t<|endoftext',\n",
       " '|>h<|endoftext|>e<|endoftext|> <|endoftext|>n<|endoftext|>e<|endoftext|>e<|endoftext|>d<|endoftext|>l<|endoftext|>e<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>s<|endoftext|>e<|endoftext|>w<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>b<|endoftext|>u<|endoftext|>t<|endoftext|>t<|endoftext|>o<|endoftext|>n<|endoftext|> <|endoftext|>o<|endoftext|>n<|endoftext|> <|endoftext|>L<|endoftext|',\n",
       " \">i<|endoftext|>l<|endoftext|>y<|endoftext|>'<|endoftext|>s<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>i<|endoftext|>r<|endoftext|>t<|endoftext|>.<|endoftext|> <|endoftext|>I<|endoftext|>t<|endoftext|> <|endoftext|>w<|endoftext|>a<|endoftext|>s<|endoftext|> <|endoftext|>n<|endoftext|>o<|endoftext|>t<|endoftext|> <|endoftext|>d<|endoftext|>i<|endoftext|>f<|endoftext|>f<|endoftext|>i<|endoftext|>c<|endoftext|>u<|endoftext|>l<|endoftext|>t<|endoftext|> <|endoftext|>f<|endoftext|>\",\n",
       " 'o<|endoftext|>r<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>m<|endoftext|> <|endoftext|>b<|endoftext|>e<|endoftext|>c<|endoftext|>a<|endoftext|>u<|endoftext|>s<|endoftext|>e<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>y<|endoftext|> <|endoftext|>w<|endoftext|>e<|endoftext|>r<|endoftext|>e<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>i<|endoftext|>n<|endoftext|>g<|endoftext|> <|endoftext|>a<|endoftext|>n',\n",
       " '<|endoftext|>d<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>l<|endoftext|>p<|endoftext|>i<|endoftext|>n<|endoftext|>g<|endoftext|> <|endoftext|>e<|endoftext|>a<|endoftext|>c<|endoftext|>h<|endoftext|> <|endoftext|>o<|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|>.<|endoftext|> <|endoftext|>A<|endoftext|>f<|endoftext|>t<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>y<|endoftext|> <|endoftext|>f<|endoftext|>i<',\n",
       " '|endoftext|>n<|endoftext|>i<|endoftext|>s<|endoftext|>h<|endoftext|>e<|endoftext|>d<|endoftext|>,<|endoftext|> <|endoftext|>L<|endoftext|>i<|endoftext|>l<|endoftext|>y<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>a<|endoftext|>n<|endoftext|>k<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>m<|endoftext|>o<|endoftext|>m<|endoftext|> <|endoftext|>f<|endoftext|>o<|endoftext|>r<|endoftext|> <|endoftext|>s<|endoftext|>h<|',\n",
       " 'endoftext|>a<|endoftext|>r<|endoftext|>i<|endoftext|>n<|endoftext|>g<|endoftext|> <|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|> <|endoftext|>n<|endoftext|>e<|endoftext|>e<|endoftext|>d<|endoftext|>l<|endoftext|>e<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>f<|endoftext|>i<|endoftext|>x<|endoftext|>i<|endoftext|>n<|endoftext|>g<|endoftext|> <|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>i<|e',\n",
       " 'ndoftext|>r<|endoftext|>t<|endoftext|>.<|endoftext|> <|endoftext|>T<|endoftext|>h<|endoftext|>e<|endoftext|>y<|endoftext|> <|endoftext|>b<|endoftext|>o<|endoftext|>t<|endoftext|>h<|endoftext|> <|endoftext|>f<|endoftext|>e<|endoftext|>l<|endoftext|>t<|endoftext|> <|endoftext|>h<|endoftext|>a<|endoftext|>p<|endoftext|>p<|endoftext|>y<|endoftext|> <|endoftext|>b<|endoftext|>e<|endoftext|>c<|endoftext|>a<|endoftext|>u<|endoftext|>s<|endoftext|>e<|endoftext|> <|endoftext|>t<|endoftext|>h<|en',\n",
       " 'doftext|>e<|endoftext|>y<|endoftext|> <|endoftext|>h<|endoftext|>a<|endoftext|>d<|endoftext|> <|endoftext|>s<|endoftext|>h<|endoftext|>a<|endoftext|>r<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>a<|endoftext|>n<|endoftext|>d<|endoftext|> <|endoftext|>w<|endoftext|>o<|endoftext|>r<|endoftext|>k<|endoftext|>e<|endoftext|>d<|endoftext|> <|endoftext|>t<|endoftext|>o<|endoftext|>g<|endoftext|>e<|endoftext|>t<|endoftext|>h<|endoftext|>e<|endoftext|>r<|endoftext|>.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   46, 50256,    77, ..., 50257, 50257, 50257])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = keep_single_column(ds[\"train\"], \"text\")\n",
    "if tokenizer.pad_token is None:\n",
    "    # We add a padding token, purely to implement the tokenizer. This will be removed before inputting tokens to the model, so we do not need to increment d_vocab in the model.\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "# Define the length to chop things up into - leaving space for a bos_token if required\n",
    "\n",
    "seq_len = max_length - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([462296, 1024])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a config dataclass that contains all the hyper-parameters\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # These numbers are not from the standard implementation of GTP-2. Instead \n",
    "    # most numbers are much smaller due to training resource constraints. Only\n",
    "    # the d_vocab is consistent, as without that the tokenizer will not work.\n",
    "    d_model = 256\n",
    "    d_head = 64\n",
    "    n_heads = 5\n",
    "    d_mlp = 1024\n",
    "    d_vocab = 50257\n",
    "    layer_norm_eps = 1e-5\n",
    "    init_std = 0.02\n",
    "    max_ctx = MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Embedding and Unembedding layers\n",
    "\n",
    "This include the embedding layer, the positional enmbedding and the final unembedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    \"\"\" \n",
    "        Embedding layer that takes as inputs a batch of \n",
    "        token and embeds them into vectors of size d_model.\n",
    "        This is the first layer after the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_vocab, d_model, init_std = 0.02):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Parameter(torch.empty(d_vocab, d_model)) # embedding matrix\n",
    "        nn.init.normal_(self.W_E, std=init_std)\n",
    "        print(self.W_E.shape)\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        # input_tokens are of size [batch_size, position] and are \n",
    "        # essentially integers that index the rows of W_E\n",
    "\n",
    "        embedded = self.W_E[input_tokens,:]\n",
    "        return embedded\n",
    "    \n",
    "class PositionEmbedding(nn.Module):\n",
    "    \"\"\" \n",
    "    \n",
    "        This layer produces the relevant positional information.\n",
    "        This positional information is then added to the output of\n",
    "        the Embedding layer resulting in the input to the first\n",
    "        transformer block \n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, max_ctx, d_model, init_std = 0.02):\n",
    "        super().__init__()\n",
    "        self.W_P = nn.Parameter(torch.empty(max_ctx, d_model))\n",
    "        nn.init.normal_(self.W_P, std = init_std)\n",
    "        print(self.W_P.shape)\n",
    "\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        #input_tokens are of size [batch, position]\n",
    "        pos_embed = repeat(self.W_P[:input_tokens.shape[1],:], \"pos d_model -> batch pos d_model\", batch = input_tokens.shape[0])\n",
    "        print(pos_embed.shape)\n",
    "        return pos_embed\n",
    "    \n",
    "class UnembeddingLayer(nn.Module):\n",
    "    \"\"\" Unembedding layer that takes as inputs the ouput\n",
    "        from the last transformer block and expands it back\n",
    "        to a vector of size d_vocab, which are the logits that\n",
    "        get passed on to the Softmax. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_vocab, d_model, init_std):\n",
    "        super().__init__()\n",
    "        self.W_U = nn.Parameter(torch.empty(d_model,d_vocab))\n",
    "        nn.init.normal_(self.W_U, std = init_std)\n",
    "        self.b_U = nn.Parameter(torch.zeros(d_vocab,))\n",
    "\n",
    "    def forward(self, resid_embed_last):\n",
    "        # resid_embed_last corresponds to the residual stream\n",
    "        # after the last transformer block. It is of size \n",
    "        # [batch_size, position, d_model] \n",
    "\n",
    "        logits = einsum(resid_embed_last, self.W_U,  \"batch pos d_model, d_model d_vocab -> batch pos d_vocab\") + self.b_U\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Layer Norm\n",
    "\n",
    "Note that using the in-built nn.LayerNorm() is maybe more conveneint. But here, I have written my own, for learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    \"\"\" Layer Normalization. Effectively z-scores it's input\n",
    "        along the embedding dimension and then multiplies \n",
    "        each embedding dimension independently by learnable\n",
    "        gains\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, init_std = 0.02, layer_norm_eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.gains = nn.Parameter(torch.empty(d_model,))\n",
    "        nn.init.normal_(self.gains,std = init_std)\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model,))\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Input is the residual stram and is a tensor of \n",
    "        # size [batch, position, d_model]. The layer norm\n",
    "        # subtracts the mean and variance computed across the\n",
    "        # d_model dimension\n",
    "        layer_mean = input.mean(dim=-1,keepdim=True)\n",
    "        input_centered = (input - layer_mean)\n",
    "        layer_var = input_centered.var(dim=-1,keepdim=True)\n",
    "        layer_scale = torch.sqrt(layer_var + self.layer_norm_eps) \n",
    "        input_normalized = input_centered/layer_scale\n",
    "        output = input_normalized *self.gains + self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    \"\"\" This module implements the self-attention mechanism with\n",
    "     multiple heads assuming that there are a total of \"n_heads\" heads.\n",
    "    The forward() method of this class provides the output of the\n",
    "    attention module which is a weighted combination of value vectors,\n",
    "    where the weights are the \"attention weights\" \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, d_model, d_head, mask_val = -1e5, init_std=0.02):\n",
    "        super().__init__()\n",
    "        self.W_Q = torch.nn.Parameter(torch.empty(n_heads, d_model, d_head))\n",
    "        nn.init.normal_(self.W_Q,std = init_std)\n",
    "        self.b_Q = torch.nn.Parameter(torch.zeros(n_heads, d_head))\n",
    "        self.W_K = torch.nn.Parameter(torch.empty(n_heads, d_model, d_head))\n",
    "        nn.init.normal_(self.W_K,std = init_std)\n",
    "        self.b_K = torch.nn.Parameter(torch.zeros(n_heads, d_head))\n",
    "        self.W_V = torch.nn.Parameter(torch.empty(n_heads, d_model, d_head))\n",
    "        nn.init.normal_(self.W_V,std = init_std)\n",
    "        self.b_V = torch.nn.Parameter(torch.zeros(n_heads, d_head))\n",
    "        self.W_O = torch.nn.Parameter(torch.empty(n_heads, d_head, d_model))\n",
    "        nn.init.normal_(self.W_O,std = init_std)\n",
    "        self.b_O = torch.nn.Parameter(torch.zeros(d_model))\n",
    "        self.register_buffer(\"mask_val\",torch.tensor(mask_val,dtype = torch.float32))\n",
    "\n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre is of size [batch, position, d_model]\n",
    "        \n",
    "        # Computing the keys, queries and values \n",
    "        keys = einsum(resid_pre, self.W_K, \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\") + self.b_K\n",
    "        queries = einsum(resid_pre, self.W_Q, \"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\") + self.b_Q\n",
    "        values = einsum(resid_pre, self.W_V, \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\") + self.b_V\n",
    "        \n",
    "        # Computing the attention pattern\n",
    "        attn_pattern = einsum(queries, keys, \"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\") \n",
    "        attn_pattern /= math.sqrt(self.W_Q.shape[-1])\n",
    "        mask = torch.triu(torch.ones(attn_pattern.shape[-2], attn_pattern.shape[-1]),diagonal=1).bool()\n",
    "        attn_pattern.masked_fill_(mask, self.mask_val)\n",
    "        attn_pattern = nn.Softmax(dim=-1)(attn_pattern)\n",
    "        context_vec = einsum(attn_pattern, values, \"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch n_heads query_pos d_head\")\n",
    "\n",
    "        # Note that in the original formulation of the transformer, one concatenates the outputs of the heads and then multiplies by a W_O that has \n",
    "        # block rows equivalent to the individual W_O for each head. This is done for efficiency purposes. Therefore, my implementation below is \n",
    "        # not efficient, but is only for didactic clarity.\n",
    "\n",
    "        # Output : Sum of all attention heads (see comment above)\n",
    "        output = einsum(context_vec, self.W_O, \"batch n_heads query_pos d_head, n_heads d_head d_model -> batch query_pos d_model\") + self.b_O\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu_new(input):\n",
    "    \"\"\"\n",
    "    This is the activation function used by GPT-2. Apparently,\n",
    "    it's slightly different from PyTorch's nn.gelu() implementation\n",
    "    \"\"\"\n",
    "    return 0.5*input* (1.0 + torch.tanh(math.sqrt(2.0/math.pi) * \n",
    "                        (input + 0.04715 *torch.pow(input, 3.0))))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_mlp, d_model):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(d_model, d_mlp)\n",
    "        self.output_layer = nn.Linear(d_mlp, d_model)\n",
    "        self.act_fn = gelu_new\n",
    "\n",
    "    def forward(self, resid_attended):\n",
    "        pre_act = self.input_layer(resid_attended)\n",
    "        act = self.act_fn(pre_act)\n",
    "        output = self.output_layer(act)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_mlp, d_head, n_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(d_model =d_model)\n",
    "        self.attn = SelfAttention(n_heads, d_model, d_head)\n",
    "        self.ln2 = LayerNorm(d_model =d_model)\n",
    "        self.mlp = MLP(d_mlp, d_model) \n",
    "\n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre is of size [batch, position, d_model]\n",
    "        ln1_out = self.ln1(resid_pre)\n",
    "        attn_out = self.attn(ln1_out)\n",
    "        resid_mid = resid_pre +  attn_out\n",
    "        ln2_out = self.ln2(resid_mid)\n",
    "        resid_post = resid_mid + self.mlp(ln2_out)\n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed = EmbeddingLayer(config.d_vocab, config.d_model, config.init_std)\n",
    "        self.pos_embed = PositionEmbedding(config.max_ctx, config.d_model, config.init_std)\n",
    "        self.transformer = nn.ModuleList([TransformerBlock(config.d_model, config.d_mlp,\n",
    "                                config.d_head, config.n_heads) for _ in config.num_blocks])\n",
    "        self.ln = LayerNorm(config.d_model, config.init_std, config.layer_norm_eps)\n",
    "        self.unembed = UnembeddingLayer(config.d_vocab, config.d_model, config.init_std)\n",
    "\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        embedded = self.embed(tokens)\n",
    "        pos_embed = self.pos_embed(tokens)\n",
    "        resid = embedded + pos_embed\n",
    "        for i in range(len(self.transformer)):\n",
    "            resid = self.transformer[i](resid)\n",
    "        resid_normalized_final = self.ln(resid)\n",
    "        logits = self.unembed(resid_normalized_final)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_lm(logits, tokens):\n",
    "    # tokens of size [batch, pos]\n",
    "    # logits of size [batch, pos, d_vocab]\n",
    "\n",
    "    log_probs = nn.LogSoftmax(dim=-1)(logits)\n",
    "    pred_log_probs = torch.gather(log_probs[:,:-1,:],dim=-1, index = tokens[:,1:].unsqueeze(dim=-1)).squeeze(dim=-1)\n",
    "    return pred_log_probs.mean(dim=-1)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechinterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
