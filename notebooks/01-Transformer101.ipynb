{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook will attempt to build a GPT-2 style transformer model from scratch\n",
    "\n",
    "The 'key' individual components are as follows:\n",
    "\n",
    "1. Embedding (Word and Position Embedding) and Unembedding Layers\n",
    "\n",
    "2. Layer Norm - This occurs before every new layer (i.e either before attention, MLP or unembed)\n",
    "\n",
    "3. A transformer block consisiting of:\n",
    "\n",
    "    i. Self-attention (usually multiple independent heads)\n",
    "\n",
    "    ii. Multi-layer Perceptron \n",
    "\n",
    "The architecture used by the transformer is a 'residual' type architecture, so the main information highway of the transformer is the \"residual stream\". The putputs of each layer just add back to the residual stream.\n",
    "\n",
    "The inputs to the model are a series of tokens these are typically an integer representation that are obtained through tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import einsum, rearrange, reduce, repeat\n",
    "import math\n",
    "import transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016f8363f530489080c68f15e4a44aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d666c6f59ad3481a99b365c4c090758a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2d5b86b20041389255e2af7224fb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9406054b9d409eb83fc1771aab5a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc96f6ee633e41559558aee95fe8bbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27a0f695d3446fe8061d22f594e4140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5628770d54124cdf997ebb8d732e20c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_reference = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Embedding and Unembedding layers\n",
    "\n",
    "This include the embedding layer, the positional enmbedding and the unembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    \"\"\" \n",
    "        Embedding layer that takes as inputs a batch of \n",
    "        token and embeds them into vectors of size d_model.\n",
    "        This is the first layer after the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_vocab, d_model, init_std = 0.02):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Parameter(torch.empty(d_vocab, d_model)) # embedding matrix\n",
    "        nn.init.normal_(self.W_E, std=init_std)\n",
    "        print(self.W_E.shape)\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        # input_tokens are of size [batch_size, position] and are \n",
    "        # essentially integers that index the rows of W_E\n",
    "\n",
    "        embedded = self.W_E[input_tokens,:]\n",
    "        return embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    \"\"\" \n",
    "    \n",
    "        This layer produces the relevant positional information.\n",
    "        This positional information is then added to the output of\n",
    "        the Embedding layer resulting in the input to the first\n",
    "        transformer block \n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, max_ctx, d_model, init_std = 0.02):\n",
    "        super().__init__()\n",
    "        self.W_P = nn.Parameter(torch.empty(max_ctx, d_model))\n",
    "        nn.init.normal_(self.W_P, std = init_std)\n",
    "        print(self.W_P.shape)\n",
    "\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        #input_tokens are of size [batch, position]\n",
    "        pos_embed = repeat(self.W_P[:input_tokens.shape[1],:], \"position d_model -> batch position d_model\", batch = input_tokens.shape[0])\n",
    "        print(pos_embed.shape)\n",
    "        return pos_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnembeddingsLayer(nn.Module):\n",
    "    \"\"\" Unembedding layer that takes as inputs the ouput\n",
    "        from the last transformer block and expands it back\n",
    "        to a vector of size d_vocab, which are the logits that\n",
    "        get passed on to the Softmax. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_vocab, d_model, init_std):\n",
    "        super().__init__()\n",
    "        self.W_U = nn.Parameter(torch.empty(d_model,d_vocab))\n",
    "        nn.init.normal_(self.W_U, std = init_std)\n",
    "        self.b_U = nn.Parameter(torch.zeros(d_vocab,))\n",
    "\n",
    "    def forward(self, resid_embed_last):\n",
    "        # resid_embed_last corresponds to the residual stream\n",
    "        # after the last transformer block. It is of size \n",
    "        # [batch_size, position, d_model] \n",
    "\n",
    "        logits = einsum(resid_embed_last, self.W_U,  \"batch position d_model, d_model d_vocab -> batch position d_vocab\") + self.b_U\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    \"\"\" Layer Normalization. Effectively z-scores it's input\n",
    "        along the embedding dimension and then multiplies \n",
    "        each embedding dimension independently by learnable\n",
    "        gains\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, init_std = 0.02, layer_norm_eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.gains = nn.Parameter(torch.empty(d_model,))\n",
    "        nn.init.normal_(self.gains,std = init_std)\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model,))\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Input is the residual stram and is a tensor of \n",
    "        # size [batch, position, d_model]. The layer norm\n",
    "        # subtracts the mean and variance computed across the\n",
    "        # d_model dimension\n",
    "        layer_mean = input.mean(dim=-1,keepdim=True)\n",
    "        print(layer_mean[0])\n",
    "        input_centered = (input - layer_mean)\n",
    "        layer_var = input_centered.var(dim=-1,keepdim=True)\n",
    "        layer_scale = torch.sqrt(layer_var + self.layer_norm_eps) \n",
    "        print(layer_scale[0])\n",
    "        input_normalized = input_centered/layer_scale\n",
    "        output = input_normalized *self.gains + self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 1024])\n",
      "tensor([[ 0.0088],\n",
      "        [ 0.0064],\n",
      "        [-0.0147],\n",
      "        [-0.0163],\n",
      "        [ 0.0269],\n",
      "        [-0.0111],\n",
      "        [-0.0473],\n",
      "        [-0.0179],\n",
      "        [ 0.0051],\n",
      "        [ 0.0409]])\n",
      "tensor([[1.0105],\n",
      "        [0.9872],\n",
      "        [0.9871],\n",
      "        [1.0257],\n",
      "        [0.9980],\n",
      "        [0.9808],\n",
      "        [1.0369],\n",
      "        [1.0059],\n",
      "        [1.0105],\n",
      "        [1.0125]])\n",
      "torch.Size([64, 10, 1024])\n"
     ]
    }
   ],
   "source": [
    "d_model = 1024\n",
    "LN1 = LayerNorm(d_model=d_model)\n",
    "batch_size, position = 64, 10\n",
    "input = torch.randn(batch_size,position,d_model)\n",
    "print(input.shape)\n",
    "output = LN1(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\" This module implements the self-attention mechanism with\n",
    "     multiple heads assuming that there are a total of \"n_heads\" heads.\n",
    "    The forward() method of this class provides the output of the\n",
    "    attention module which is a weighted combination of value vectors,\n",
    "    where the weights are the \"attention weights\" \"\"\"\n",
    "    def __init__(self, n_heads, d_model, d_head, mask_val = -1e5, init_std=0.02):\n",
    "        super().__init__()\n",
    "        self.W_Q = torch.nn.Parameter(torch.empty(n_heads, d_model, d_head))\n",
    "        nn.init.normal_(self.W_Q,std = init_std)\n",
    "        self.b_Q = torch.nn.Parameter(torch.zeros(n_heads, d_head))\n",
    "        self.W_K = torch.nn.Parameter(torch.empty(n_heads, d_model, d_head))\n",
    "        nn.init.normal_(self.W_K,std = init_std)\n",
    "        self.b_K = torch.nn.Parameter(torch.zeros(n_heads, d_head))\n",
    "        self.W_V = torch.nn.Parameter(torch.empty(n_heads, d_model, d_head))\n",
    "        self.b_V = torch.nn.Parameter(torch.zeros(n_heads, d_head))\n",
    "        nn.init.normal_(self.W_V,std = init_std)\n",
    "        self.W_O = torch.nn.Parameter(torch.empty(n_heads, d_head, d_model))\n",
    "        nn.init.normal_(self.W_O,std = init_std)\n",
    "        self.b_O = torch.nn.Parameter(torch.zeros(d_model))\n",
    "        self.register_buffer(\"mask_val\",torch.tensor(mask_val,dtype = torch.float32))\n",
    "\n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre is of size [batch, position, d_model]\n",
    "        keys = einsum(resid_pre, self.W_K, \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\") + self.b_K\n",
    "        print(f'Keys of size:{keys.shape}')\n",
    "        queries = einsum(resid_pre, self.W_Q, \"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\") + self.b_Q\n",
    "        print(f'Queries of size:{queries.shape}')\n",
    "        values = einsum(resid_pre, self.W_V, \"batch val_pos d_model, n_heads d_model d_head -> batch val_pos n_heads d_head\") + self.b_V\n",
    "        print(f'Values of size:{values.shape}')\n",
    "        attn_pattern = einsum(queries, keys, \"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\") \n",
    "        attn_pattern /= math.sqrt(self.W_Q.shape[-1])\n",
    "\n",
    "        print(f'Shape of attention pattern:{attn_pattern.shape}')\n",
    "        attn_pattern = self.apply_causal_mask(attn_pattern)\n",
    "        print(attn_pattern[0,0])\n",
    "        context_vec = einsum(attn_pattern, values, \"batch n_heads query_pos key_pos\", \"batch val_pos n_heads d_head -> batch n_heads query_pos d_head\")\n",
    "        print(context_vec.shape)\n",
    "\n",
    "    def apply_causal_mask(self, attn_pattern):\n",
    "        mask = torch.triu(torch.ones(attn_pattern.shape[-2], attn_pattern.shape[-1]),diagonal=1).bool()\n",
    "        print(mask)\n",
    "        attn_pattern.masked_fill_(mask, self.mask_val)\n",
    "        return attn_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_heads = SelfAttention(n_heads = 2, d_model = 1024, d_head = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of size:torch.Size([64, 10, 2, 64])\n",
      "Queries of size:torch.Size([64, 10, 2, 64])\n",
      "Values of size:torch.Size([64, 10, 2, 64])\n",
      "Shape of attention pattern:torch.Size([64, 2, 10, 10])\n",
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False]])\n",
      "tensor([[ 2.0982e-01, -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05,\n",
      "         -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05],\n",
      "        [-7.8841e-01,  1.7404e-01, -1.0000e+05, -1.0000e+05, -1.0000e+05,\n",
      "         -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05],\n",
      "        [ 2.8751e-01, -1.1390e-02, -3.2362e-01, -1.0000e+05, -1.0000e+05,\n",
      "         -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05],\n",
      "        [ 9.0963e-02, -3.0909e-01,  3.4086e-01,  3.1355e-01, -1.0000e+05,\n",
      "         -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05],\n",
      "        [ 2.2558e-01, -3.2957e-02, -1.5517e-01,  2.4928e-01,  7.2074e-01,\n",
      "         -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05],\n",
      "        [-8.4436e-01, -2.0619e-01, -1.8713e-02,  4.4991e-01,  3.0680e-01,\n",
      "         -1.5269e-01, -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05],\n",
      "        [ 3.4804e-01,  9.7660e-01, -8.4313e-01,  4.1722e-01,  4.4696e-02,\n",
      "          3.8889e-01, -1.6853e-01, -1.0000e+05, -1.0000e+05, -1.0000e+05],\n",
      "        [ 1.8414e-01, -5.3982e-02,  1.5014e-01,  1.7834e-01, -1.8272e-01,\n",
      "         -7.0910e-01,  3.4112e-01,  1.5940e-01, -1.0000e+05, -1.0000e+05],\n",
      "        [ 5.6558e-01, -7.1603e-03,  1.9966e-01, -2.9574e-01, -3.1836e-01,\n",
      "         -3.6092e-01,  2.9820e-01,  1.5592e-01, -6.5647e-01, -1.0000e+05],\n",
      "        [-2.6187e-01, -3.3594e-02, -9.8390e-01,  5.6126e-02,  2.0465e-01,\n",
      "          5.4291e-01,  2.0200e-01, -4.3701e-01,  6.2825e-01,  1.0301e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "EinopsError",
     "evalue": "Unknown axis query_pos on right side of einsum batch val_pos n_heads d_head -> batch n_heads query_pos d_head.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m batch,position \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      2\u001b[0m resid_pre \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch,position,\u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mattn_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresid_pre\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mechinterp/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mechinterp/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[95], line 37\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, resid_pre)\u001b[0m\n\u001b[1;32m     35\u001b[0m attn_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_causal_mask(attn_pattern)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(attn_pattern[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 37\u001b[0m context_vec \u001b[38;5;241m=\u001b[39m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch n_heads query_pos key_pos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch val_pos n_heads d_head -> batch n_heads query_pos d_head\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(context_vec\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/mechinterp/lib/python3.11/site-packages/einops/einops.py:900\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*tensors_and_pattern)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe last argument passed to `einops.einsum` must be a string, representing the einsum pattern.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m tensors \u001b[38;5;241m=\u001b[39m tensors_and_pattern[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 900\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[43m_compactify_pattern_for_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_backend(tensors[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39meinsum(pattern, \u001b[38;5;241m*\u001b[39mtensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/mechinterp/lib/python3.11/site-packages/einops/einops.py:799\u001b[0m, in \u001b[0;36m_compactify_pattern_for_einsum\u001b[0;34m(pattern)\u001b[0m\n\u001b[1;32m    796\u001b[0m     axis_name \u001b[38;5;241m=\u001b[39m raw_axis_name[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m axis_name_mapping:\n\u001b[0;32m--> 799\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on right side of einsum \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    801\u001b[0m     compact_pattern \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m axis_name_mapping[axis_name]\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compact_pattern\n",
      "\u001b[0;31mEinopsError\u001b[0m: Unknown axis query_pos on right side of einsum batch val_pos n_heads d_head -> batch n_heads query_pos d_head."
     ]
    }
   ],
   "source": [
    "batch,position = 64,10\n",
    "resid_pre = torch.randn(batch,position,1024)\n",
    "attn_heads(resid_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(attn_heads.mask_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechinterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
